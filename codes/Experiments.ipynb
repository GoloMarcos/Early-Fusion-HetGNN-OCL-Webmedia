{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3900,"status":"ok","timestamp":1684014471859,"user":{"displayName":"Marcos Paulo Silva Gôlo","userId":"00854347238887874462"},"user_tz":180},"id":"4kdMfefYfrS3","outputId":"c792a528-6570-469b-f5f9-4da3e3707a95"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","\n","drive.mount(\"/content/drive\", force_remount = True)"]},{"cell_type":"code","source":["!python -V"],"metadata":{"id":"ECMybVG-C3SS","executionInfo":{"status":"ok","timestamp":1684785574826,"user_tz":180,"elapsed":315,"user":{"displayName":"Marcos Paulo Silva Gôlo","userId":"00854347238887874462"}},"outputId":"31207972-f5fb-4fda-d169-e4a374ea962b","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Python 3.10.11\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"femrMVlsgCy4"},"outputs":[],"source":["PATH_TO_GAE_GRAPHS = \"/content/drive/MyDrive/USP/Doctorate/Research/Articles/Early Fusion For One-class Learning On Heterogeneous Graphs/grafos_gae\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":988,"status":"ok","timestamp":1684014490536,"user":{"displayName":"Marcos Paulo Silva Gôlo","userId":"00854347238887874462"},"user_tz":180},"id":"4TINhKmwhpqx","outputId":"3cf34cf5-fe4f-4223-f0b7-aaa20fdb1e18"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'hidden': [32], 'lr': 0.01, 'patience': 50},\n"," {'hidden': [32], 'lr': 0.01, 'patience': 100},\n"," {'hidden': [32], 'lr': 0.001, 'patience': 50},\n"," {'hidden': [32], 'lr': 0.001, 'patience': 100},\n"," {'hidden': [32], 'lr': 0.0001, 'patience': 50},\n"," {'hidden': [32], 'lr': 0.0001, 'patience': 100},\n"," {'hidden': [64], 'lr': 0.01, 'patience': 50},\n"," {'hidden': [64], 'lr': 0.01, 'patience': 100},\n"," {'hidden': [64], 'lr': 0.001, 'patience': 50},\n"," {'hidden': [64], 'lr': 0.001, 'patience': 100},\n"," {'hidden': [64], 'lr': 0.0001, 'patience': 50},\n"," {'hidden': [64], 'lr': 0.0001, 'patience': 100},\n"," {'hidden': [32, 32], 'lr': 0.01, 'patience': 50},\n"," {'hidden': [32, 32], 'lr': 0.01, 'patience': 100},\n"," {'hidden': [32, 32], 'lr': 0.001, 'patience': 50},\n"," {'hidden': [32, 32], 'lr': 0.001, 'patience': 100},\n"," {'hidden': [32, 32], 'lr': 0.0001, 'patience': 50},\n"," {'hidden': [32, 32], 'lr': 0.0001, 'patience': 100},\n"," {'hidden': [64, 64], 'lr': 0.01, 'patience': 50},\n"," {'hidden': [64, 64], 'lr': 0.01, 'patience': 100},\n"," {'hidden': [64, 64], 'lr': 0.001, 'patience': 50},\n"," {'hidden': [64, 64], 'lr': 0.001, 'patience': 100},\n"," {'hidden': [64, 64], 'lr': 0.0001, 'patience': 50},\n"," {'hidden': [64, 64], 'lr': 0.0001, 'patience': 100}]"]},"metadata":{},"execution_count":5}],"source":["from sklearn.model_selection import ParameterGrid\n","\n","HYPERPARAMETER_GRID_GAE = {\n","    'hidden': [[32], [64], [32, 32], [64, 64]],\n","    'lr': [1e-2, 1e-3, 1e-4],\n","    'patience': [50, 100]\n","}\n","\n","hyperparameter_list_gae = list(ParameterGrid(HYPERPARAMETER_GRID_GAE))\n","hyperparameter_list_gae"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":249,"status":"ok","timestamp":1684014492179,"user":{"displayName":"Marcos Paulo Silva Gôlo","userId":"00854347238887874462"},"user_tz":180},"id":"t6Aicp85ga1B","outputId":"fc1832b5-873a-4c9e-9465-e0058b06948d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['rec_sys_1',\n"," 'rec_sys_3',\n"," 'rec_sys_2',\n"," 'rec_sys_4',\n"," 'event',\n"," 'rec_sys_5',\n"," 'music',\n"," 'fakenews']"]},"metadata":{},"execution_count":6}],"source":["import os\n","\n","graphs = os.listdir(PATH_TO_GAE_GRAPHS)\n","graphs"]},{"cell_type":"code","source":["define_nus()"],"metadata":{"id":"OH5FFGZtiMxC","executionInfo":{"status":"ok","timestamp":1685565529183,"user_tz":180,"elapsed":12,"user":{"displayName":"Marcos Paulo Silva Gôlo","userId":"00854347238887874462"}},"outputId":"8ac55f3e-6fe5-49bd-8954-81aab6cd2849","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0.05,\n"," 0.1,\n"," 0.15,\n"," 0.2,\n"," 0.25,\n"," 0.3,\n"," 0.35,\n"," 0.4,\n"," 0.45,\n"," 0.5,\n"," 0.55,\n"," 0.6,\n"," 0.65,\n"," 0.7,\n"," 0.75,\n"," 0.8,\n"," 0.85,\n"," 0.005,\n"," 0.01,\n"," 0.015,\n"," 0.02,\n"," 0.025,\n"," 0.03,\n"," 0.035,\n"," 0.04,\n"," 0.045,\n"," 0.05,\n"," 0.055,\n"," 0.06,\n"," 0.065,\n"," 0.07,\n"," 0.075,\n"," 0.08,\n"," 0.085]"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4wmHQirQ3n6E"},"outputs":[],"source":["from pandas.core.base import value_counts\n","from sklearn.metrics import classification_report, roc_auc_score\n","import numpy as np\n","import time\n","from pathlib import Path\n","from sklearn.svm import OneClassSVM as OCSVM\n","from sklearn.model_selection import KFold\n","\n","def define_gammas():\n","  gammas = ['scale', 'auto']\n","  return gammas\n","\n","def define_nus():\n","  nus = []\n","  for n in range(5,90,5):\n","    nus.append(n/100)\n","  for n in range(5,90,5):\n","    nus.append(n/1000)\n","\n","  return nus\n","\n","def define_kernels():\n","  return ['rbf', 'sigmoid', 'linear', 'poly']\n","\n","def evaluation_one_class(preds_interest, preds_outliers):\n","    y_true = [1] * len(preds_interest) + [-1] * len(preds_outliers)\n","    y_pred = list(preds_interest) + list(preds_outliers)\n","    return classification_report(y_true, y_pred, output_dict=True)\n","\n","def evaluate_model(X_train, X_test, X_outlier, model):\n","\n","    one_class_classifier = model.fit(X_train)\n","\n","    Y_pred_interest = one_class_classifier.predict(X_test)\n","\n","    Y_pred_ruido = one_class_classifier.predict(X_outlier)\n","\n","    y_true = np.array([1] * len(X_test) + [-1] * len(X_outlier))\n","\n","    dic = evaluation_one_class(Y_pred_interest, Y_pred_ruido)\n","\n","    return dic\n","\n","def init_metrics():\n","    metrics = {\n","        '1': {\n","            'precision': [],\n","            'recall': [],\n","            'f1-score': []\n","        },\n","        '-1': {\n","            'precision': [],\n","            'recall': [],\n","            'f1-score': []\n","        },\n","        'macro avg': {\n","            'precision': [],\n","            'recall': [],\n","            'f1-score': []\n","        },\n","        'weighted avg': {\n","            'precision': [],\n","            'recall': [],\n","            'f1-score': []\n","        },\n","        'accuracy': [],\n","        'time': []\n","    }\n","    return metrics\n","\n","\n","def save_values(metrics, values):\n","    for key in metrics.keys():\n","      if key == 'accuracy' or key == 'time':\n","        metrics[key].append(values[key])\n","      else:\n","        for key2 in metrics[key].keys():\n","          metrics[key][key2].append(values[key][key2])\n","\n","\n","def operation(graph, list_nodes, representation_name, operador, dataset):\n","  x = []\n","\n","  for node in list_nodes:\n","    rep = graph.nodes[node][representation_name]\n","\n","    if dataset == 'event' :\n","      dic_rep = {'what' : [],\n","                 'when' : [],\n","                 'where' : [],\n","                 'who' : [],\n","                 'how' : [],\n","                 'cluster_code' : [],\n","                 'iptc_code': []}\n","\n","      for n in graph.neighbors(node):\n","        for key in dic_rep:\n","          if key in n:\n","             dic_rep[key].append(graph.nodes[n][representation_name])\n","\n","      for key in dic_rep:\n","        dic_rep[key] = np.mean(dic_rep[key], axis=0)\n","\n","      new_rep=None\n","      if operator == 'concatenate':\n","        new_rep = np.concatenate([rep, dic_rep['what'], dic_rep['when'], dic_rep['where'], dic_rep['who'], dic_rep['how'], dic_rep['cluster_code'], dic_rep['iptc_code']])\n","      elif operator == 'sum':\n","        new_rep = np.sum([rep, dic_rep['what'], dic_rep['when'], dic_rep['where'], dic_rep['who'], dic_rep['how'], dic_rep['cluster_code'], dic_rep['iptc_code']], axis=0)\n","      elif operator == 'sub':\n","        new_rep = rep\n","        for key in dic_rep:\n","          new_rep = np.subtract(new_rep, dic_rep[key])\n","      elif operator == 'avg':\n","        new_rep = np.mean([rep, dic_rep['what'], dic_rep['when'], dic_rep['where'], dic_rep['who'], dic_rep['how'], dic_rep['cluster_code'], dic_rep['iptc_code']], axis=0)\n","      elif operator == 'min':\n","        new_rep = np.min([rep, dic_rep['what'], dic_rep['when'], dic_rep['where'], dic_rep['who'], dic_rep['how'], dic_rep['cluster_code'], dic_rep['iptc_code']], axis=0)\n","      elif operator == 'max':\n","        new_rep = np.max([rep, dic_rep['what'], dic_rep['when'], dic_rep['where'], dic_rep['who'], dic_rep['how'], dic_rep['cluster_code'], dic_rep['iptc_code']], axis=0)\n","      elif operator == 'multiply':\n","        new_rep = rep\n","        for key in dic_rep:\n","          new_rep = np.multiply(new_rep, dic_rep[key])\n","\n","    else:\n","      reps = []\n","      for n in graph.neighbors(node):\n","        reps.append(graph.nodes[n][representation_name])\n","\n","      rep_o = np.mean(reps, axis=0)\n","\n","      new_rep=None\n","      if operator == 'concatenate':\n","        new_rep = np.concatenate([rep, rep_o])\n","      elif operator == 'sum':\n","        new_rep = np.sum([rep, rep_o], axis=0)\n","      elif operator == 'sub':\n","        new_rep = np.subtract(rep, rep_o)\n","      elif operator == 'avg':\n","        new_rep = np.mean([rep, rep_o], axis=0)\n","      elif operator == 'min':\n","        new_rep = np.min([rep, rep_o], axis=0)\n","      elif operator == 'max':\n","        new_rep = np.max([rep, rep_o], axis=0)\n","      elif operator == 'multiply':\n","        new_rep = np.multiply(rep, rep_o)\n","\n","    x.append(new_rep)\n","\n","  return x\n","\n","def extract_emb_from_graph(graph, representation_name, interest_class, nodes_train, nodes_test, nodes_out, operator, principal_node, dataset):\n","\n","  if operator == 'without':\n","    x_train, x_int_test, x_nint_test = [], [], []\n","\n","    for node in nodes_train:\n","      x_train.append(graph.nodes[node][representation_name])\n","\n","    for node in nodes_test:\n","      x_int_test.append(graph.nodes[node][representation_name])\n","\n","    for node in nodes_out:\n","      x_nint_test.append(graph.nodes[node][representation_name])\n","  else:\n","    x_train = operation(graph, nodes_train, representation_name, operator, dataset)\n","    x_int_test = operation(graph, nodes_test, representation_name, operator, dataset)\n","    x_nint_test = operation(graph, nodes_out, representation_name, operator, dataset)\n","\n","  return x_train, x_int_test, x_nint_test\n","\n","def evaluate_models(G, representation_name, path, file_name, hyperparams_gae, interest_class, l_int, l_nint, operator, principal_node, dataset):\n","\n","    kf = KFold(n_splits=5, shuffle=True, random_state=81)\n","\n","    for kernel in define_kernels():\n","      for gamma in define_gammas():\n","        for nu in define_nus():\n","          ocsvm = OCSVM(kernel=kernel,nu=nu,gamma=gamma)\n","          line_parameters = str(hyperparams_gae) + '_kernel:' + kernel + '_gamma:' + gamma + '_nu:' + str(nu)\n","          metrics = init_metrics()\n","\n","          for train_index, test_index in kf.split(l_int):\n","            nodes_train = np.array(l_int)[train_index]\n","            nodes_test = np.array(l_int)[test_index]\n","\n","            x_train, x_int_test,x_nint_test = extract_emb_from_graph(G, representation_name, interest_class, nodes_train, nodes_test, l_nint, operator, principal_node, dataset)\n","\n","            start = time.time()\n","            values = evaluate_model(x_train, x_int_test, x_nint_test, ocsvm)\n","            end = time.time()\n","            time_ = end - start\n","            values['time'] = time_\n","            save_values(metrics, values)\n","\n","          write_results(metrics, file_name, line_parameters, path)\n","\n","\n","def write_results(metrics, file_name, line_parameters, path):\n","    if not Path(path + file_name).is_file():\n","        file_ = open(path + file_name, 'w')\n","        string = 'Parameters'\n","        for key in metrics.keys():\n","            if key == 'accuracy' or key == 'time':\n","              string += ';' + key + '-mean;' + key + '-std'\n","            else:\n","              for key2 in metrics[key].keys():\n","                string += ';' + key + '_' + key2 + '-mean;' + key + '_' + key2 + '-std'\n","\n","        string += '\\n'\n","        file_.write(string)\n","        file_.close()\n","\n","    file_ = open(path + file_name, 'a')\n","    string = line_parameters\n","\n","    for key in metrics.keys():\n","      if key == 'accuracy' or key == 'time':\n","        string += ';' + str(np.mean(metrics[key])) + ';' + str(np.std(metrics[key]))\n","      else:\n","        for key2 in metrics[key].keys():\n","          string += ';' + str(np.mean(metrics[key][key2])) + ';' + str(np.std(metrics[key][key2]))\n","\n","    string += '\\n'\n","    file_.write(string)\n","    file_.close()\n"]},{"cell_type":"markdown","metadata":{"id":"o-2jETIK3Op4"},"source":["# Music"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e8ughtveIxWD"},"outputs":[],"source":["import pickle as pkl\n","import networkx as nx\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","path_results = '/content/drive/MyDrive/USP/Doctorate/Research/Articles/Early Fusion For One-class Learning On Heterogeneous Graphs/resultados/'\n","\n","dataset = 'music'\n","\n","interest_class = 'hit'\n","\n","principal_node = 'song'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hbKKN0FD3R38"},"outputs":[],"source":["for operator in ['without', 'sum', 'sub', 'avg', 'min', 'max', 'multiply', 'concatenate']:\n","  for hyperparams_gae, pickle in zip(hyperparameter_list_gae, os.listdir(f\"{PATH_TO_GAE_GRAPHS}/{dataset}\")):\n","    print(f\"Dataset: {dataset} - GAE Hyperparameters: {hyperparams_gae}\")\n","    with open(f\"{PATH_TO_GAE_GRAPHS}/{dataset}/{pickle}\", \"rb\") as file:\n","      G = pkl.load(file)\n","\n","      l_int, l_nint = [], []\n","      for node in G.nodes():\n","        if principal_node in node and G.nodes[node]['label'] == interest_class:\n","          l_int.append(node)\n","        elif principal_node in node and G.nodes[node]['label'] != interest_class:\n","          l_nint.append(node)\n","\n","      evaluate_models(G, 'f_features', path_results, dataset + '_' + operator + '.csv', 'reg', interest_class, l_int, l_nint, operator, principal_node, dataset)\n","      break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dfQIdHTJFpYf"},"outputs":[],"source":["for operator in ['without', 'sum', 'sub', 'avg', 'min', 'max', 'multiply', 'concatenate']:\n","  print(operator)\n","  for hyperparams_gae, pickle in zip(hyperparameter_list_gae, os.listdir(f\"{PATH_TO_GAE_GRAPHS}/{dataset}\")):\n","    print(f\"Dataset: {dataset} - GAE Hyperparameters: {hyperparams_gae}\")\n","    with open(f\"{PATH_TO_GAE_GRAPHS}/{dataset}/{pickle}\", \"rb\") as file:\n","      G = pkl.load(file)\n","\n","      l_int, l_nint = [], []\n","      for node in G.nodes():\n","        if principal_node in node and G.nodes[node]['label'] == interest_class:\n","          l_int.append(node)\n","        elif principal_node in node and G.nodes[node]['label'] != interest_class:\n","          l_nint.append(node)\n","\n","      evaluate_models(G, 'gae_features', path_results, dataset + '-gae_' + operator + '.csv', 'reg', interest_class, l_int, l_nint, operator, principal_node, dataset)"]},{"cell_type":"markdown","metadata":{"id":"bt-tYmzUZRa7"},"source":["# fakenews\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TkizgDEGPlbj"},"outputs":[],"source":["import pickle as pkl\n","import networkx as nx\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","path_results = '/content/drive/MyDrive/USP/Doctorate/Research/Articles/Early Fusion For One-class Learning On Heterogeneous Graphs/resultados/'\n","\n","dataset = 'fakenews'\n","\n","interest_class = 'fake'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UXQLECkJZTMO"},"outputs":[],"source":["for operator in ['concatenate', 'without', 'sum', 'sub', 'avg', 'min', 'max', 'multiply']:\n","  for hyperparams_gae, pickle in zip(hyperparameter_list_gae, os.listdir(f\"{PATH_TO_GAE_GRAPHS}/{dataset}\")):\n","    print(f\"Dataset: {dataset} - GAE Hyperparameters: {hyperparams_gae}\")\n","    with open(f\"{PATH_TO_GAE_GRAPHS}/{dataset}/{pickle}\", \"rb\") as file:\n","      G = pkl.load(file)\n","\n","      l_int, l_nint = [], []\n","      for node in G.nodes():\n","        if type(node) == int and G.nodes[node]['label'] == interest_class:\n","          l_int.append(node)\n","        elif type(node) == int and G.nodes[node]['label'] != interest_class:\n","          l_nint.append(node)\n","\n","      evaluate_models(G, 'f_features', path_results, dataset + '_' + operator + '.csv', 'reg', interest_class, l_int, l_nint, operator, '', dataset)\n","      break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sgbBW0oAGbkq"},"outputs":[],"source":["for operator in ['without', 'sum', 'sub', 'avg', 'min', 'max', 'multiply', 'concatenate']:\n","  for hyperparams_gae, pickle in zip(hyperparameter_list_gae, os.listdir(f\"{PATH_TO_GAE_GRAPHS}/{dataset}\")):\n","    if len(hyperparams_gae['hidden']) == 2 and hyperparams_gae['hidden'][0] == 64:\n","      print(f\"Dataset: {dataset} - GAE Hyperparameters: {hyperparams_gae}\")\n","      with open(f\"{PATH_TO_GAE_GRAPHS}/{dataset}/{pickle}\", \"rb\") as file:\n","        G = pkl.load(file)\n","\n","        l_int, l_nint = [], []\n","        for node in G.nodes():\n","          if type(node) == int and G.nodes[node]['label'] == interest_class:\n","            l_int.append(node)\n","          elif type(node) == int and G.nodes[node]['label'] != interest_class:\n","            l_nint.append(node)\n","\n","        evaluate_models(G, 'gae_features', path_results, dataset + '-gae_' + operator + '.csv', 'reg', interest_class, l_int, l_nint, operator, '', dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"aeqKNDb33iYU","outputId":"45297650-0ff7-4b48-e3a9-a33031e401b1"},"outputs":[{"name":"stdout","output_type":"stream","text":["multiply\n","Dataset: fakenews - GAE Hyperparameters: {'hidden': [64, 64], 'lr': 0.01, 'patience': 50}\n","Dataset: fakenews - GAE Hyperparameters: {'hidden': [64, 64], 'lr': 0.01, 'patience': 100}\n","Dataset: fakenews - GAE Hyperparameters: {'hidden': [64, 64], 'lr': 0.001, 'patience': 50}\n","Dataset: fakenews - GAE Hyperparameters: {'hidden': [64, 64], 'lr': 0.001, 'patience': 100}\n","Dataset: fakenews - GAE Hyperparameters: {'hidden': [64, 64], 'lr': 0.0001, 'patience': 50}\n","Dataset: fakenews - GAE Hyperparameters: {'hidden': [64, 64], 'lr': 0.0001, 'patience': 100}\n"]}],"source":["for operator in ['without', 'sum', 'sub', 'avg', 'min', 'max', 'multiply', 'concatenate']:\n","  print(operator)\n","  for hyperparams_gae, pickle in zip(hyperparameter_list_gae, os.listdir(f\"{PATH_TO_GAE_GRAPHS}/{dataset}\")):\n","    if len(hyperparams_gae['hidden']) == 2 and hyperparams_gae['hidden'][0] == 64:\n","      print(f\"Dataset: {dataset} - GAE Hyperparameters: {hyperparams_gae}\")\n","      with open(f\"{PATH_TO_GAE_GRAPHS}/{dataset}/{pickle}\", \"rb\") as file:\n","        G = pkl.load(file)\n","\n","        l_int, l_nint = [], []\n","        for node in G.nodes():\n","          if type(node) == int and G.nodes[node]['label'] == interest_class:\n","            l_int.append(node)\n","          elif type(node) == int and G.nodes[node]['label'] != interest_class:\n","            l_nint.append(node)\n","\n","        evaluate_models(G, 'gae_features', path_results, dataset + '-gae_' + operator + '.csv', 'reg', interest_class, l_int, l_nint, operator, '', dataset)"]},{"cell_type":"markdown","metadata":{"id":"7NM_eW-kG1ov"},"source":["# Eventos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YePyExNmG4_M"},"outputs":[],"source":["import pickle as pkl\n","import networkx as nx\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","path_results = '/content/drive/MyDrive/USP/Doctorate/Research/Articles/Early Fusion For One-class Learning On Heterogeneous Graphs/resultados/'\n","\n","dataset = 'event'\n","\n","interest_class = 'f1'\n","\n","principal_node = 'event'\n","\n","for operator in ['concatenate', 'min', 'without', 'sum', 'sub', 'avg', 'max', 'multiply']:\n","  for hyperparams_gae, pickle in zip(hyperparameter_list_gae, os.listdir(f\"{PATH_TO_GAE_GRAPHS}/{dataset}\")):\n","    print(f\"Dataset: {dataset} - GAE Hyperparameters: {hyperparams_gae}\")\n","    with open(f\"{PATH_TO_GAE_GRAPHS}/{dataset}/{pickle}\", \"rb\") as file:\n","      G = pkl.load(file)\n","\n","      l_int, l_nint = [], []\n","      for node in G.nodes():\n","        if principal_node in node and G.nodes[node]['label'] == interest_class:\n","          l_int.append(node)\n","        elif principal_node in node and G.nodes[node]['label'] != interest_class:\n","          l_nint.append(node)\n","\n","      evaluate_models(G, 'f_features', path_results, dataset + '_' + operator + '.csv', 'reg', interest_class, l_int, l_nint, operator, principal_node, dataset)\n","      break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3HPiwgsbJx8N"},"outputs":[],"source":["for operator in ['without', 'sum', 'sub', 'avg', 'min', 'max', 'multiply', 'concatenate']:\n","  print(operator)\n","  for hyperparams_gae, pickle in zip(hyperparameter_list_gae, os.listdir(f\"{PATH_TO_GAE_GRAPHS}/{dataset}\")):\n","    print(f\"Dataset: {dataset} - GAE Hyperparameters: {hyperparams_gae}\")\n","    with open(f\"{PATH_TO_GAE_GRAPHS}/{dataset}/{pickle}\", \"rb\") as file:\n","      G = pkl.load(file)\n","\n","      l_int, l_nint = [], []\n","      for node in G.nodes():\n","        if principal_node in node and G.nodes[node]['label'] == interest_class:\n","          l_int.append(node)\n","        elif principal_node in node and G.nodes[node]['label'] != interest_class:\n","          l_nint.append(node)\n","\n","      evaluate_models(G, 'gae_features', path_results, dataset + '-gae_' + operator + '.csv', 'reg', interest_class, l_int, l_nint, operator, principal_node, dataset)"]},{"cell_type":"markdown","metadata":{"id":"zM0-syZtJ7Rv"},"source":["# Rec Sys"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EPw4B99wMkiX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684014510268,"user_tz":180,"elapsed":2660,"user":{"displayName":"Marcos Paulo Silva Gôlo","userId":"00854347238887874462"}},"outputId":"66838e93-7b85-46b8-e587-f000f7bff409"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading...\n","From: https://drive.google.com/uc?id=1XFiH0-J1r9DepyhfAzQUCe8pIClddNHw\n","To: /content/df_interest.pkl\n","100% 624k/624k [00:00<00:00, 24.6MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1U-qJ0Aayp2srzlIxiztEpTjIolp9fWya\n","To: /content/df_outlier.pkl\n","100% 171k/171k [00:00<00:00, 80.7MB/s]\n"]}],"source":["!gdown 1XFiH0-J1r9DepyhfAzQUCe8pIClddNHw\n","!gdown 1U-qJ0Aayp2srzlIxiztEpTjIolp9fWya"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IwX9qHC7NT2Z"},"outputs":[],"source":["import pandas as pd\n","\n","df_int = pd.read_pickle('df_interest.pkl')\n","df_out = pd.read_pickle('df_outlier.pkl')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r93t9-UCNO5k"},"outputs":[],"source":["def train_test_split_ocl_recommendation(kf, df_int):\n","    train_test = []\n","\n","    for train_index, test_index in kf.split(df_int):\n","        train_test.append((df_int.iloc[train_index], df_int.iloc[test_index]))\n","\n","    return train_test\n","\n","def foldValidation(folds):\n","    kf = KFold(n_splits=folds, shuffle=True, random_state=42)\n","    return kf\n","\n","def operation(graph, representation_name, operator, dataset, df_train, df):\n","\n","  users = df_train['user'].unique()\n","  items = df_train['item'].unique()\n","\n","  x = []\n","\n","  for index,row in df.iterrows():\n","    continue_ = 0\n","    user = str(row['user']) + ':user'\n","    item = str(row['item']) + ':item'\n","\n","    if user in graph.nodes() and item in graph.nodes():\n","      rep_i = graph.nodes[item][representation_name]\n","\n","      rep_u = graph.nodes[user][representation_name]\n","\n","      dic_rep = {'review' : [],\n","                  'genre' : [],\n","                  'keyword' : []}\n","\n","      for n in graph.neighbors(item):\n","        for key in dic_rep:\n","          if key in n:\n","            dic_rep[key].append(graph.nodes[n][representation_name])\n","\n","      for key in dic_rep:\n","        if len(dic_rep[key]) == 0:\n","          continue_ = 1\n","        else:\n","          dic_rep[key] = np.mean(dic_rep[key], axis=0)\n","\n","      if continue_ == 0 :\n","        new_rep=None\n","        if operator == 'concatenate':\n","          new_rep = np.concatenate([rep_i, rep_u, dic_rep['review'], dic_rep['genre'], dic_rep['keyword']])\n","        elif operator == 'sum':\n","          new_rep = np.sum([rep_i, rep_u, dic_rep['review'], dic_rep['genre'], dic_rep['keyword']], axis=0)\n","        elif operator == 'sub':\n","          new_rep = np.subtract(rep_i, rep_u)\n","          for key in dic_rep:\n","            new_rep = np.subtract(new_rep, dic_rep[key])\n","        elif operator == 'avg':\n","          new_rep = np.mean([rep_i, rep_u, dic_rep['review'], dic_rep['genre'], dic_rep['keyword']], axis=0)\n","        elif operator == 'min':\n","          new_rep = np.min([rep_i, rep_u, dic_rep['review'], dic_rep['genre'], dic_rep['keyword']], axis=0)\n","        elif operator == 'max':\n","          new_rep = np.max([rep_i, rep_u, dic_rep['review'], dic_rep['genre'], dic_rep['keyword']], axis=0)\n","        elif operator == 'multiply':\n","          new_rep = np.multiply(rep_i, rep_u)\n","          for key in dic_rep:\n","            new_rep = np.multiply(new_rep, dic_rep[key])\n","\n","        x.append(new_rep)\n","\n","  return x\n","\n","def extract_emb_from_graph(graph, representation_name, operator, df_train, df_test, df_out):\n","\n","  users = df_train['user'].unique()\n","  items = df_train['item'].unique()\n","\n","  if operator == 'without':\n","    x_train, x_int_test, x_nint_test = [], [], []\n","\n","    for index,row in df_train.iterrows():\n","      user = str(row['user']) + ':user'\n","      item = str(row['item']) + ':item'\n","      if user in graph.nodes() and item in graph.nodes():\n","        x_train.append(np.concatenate([graph.nodes[user][representation_name], graph.nodes[item][representation_name]]))\n","\n","    for index,row in df_test.iterrows():\n","      user = str(row['user']) + ':user'\n","      item = str(row['item']) + ':item'\n","      if user in graph.nodes() and item in graph.nodes():\n","        x_int_test.append(np.concatenate([graph.nodes[user][representation_name], graph.nodes[item][representation_name]]))\n","\n","    for index,row in df_out.iterrows():\n","      user = str(row['user']) + ':user'\n","      item = str(row['item']) + ':item'\n","      if user in graph.nodes() and item in graph.nodes():\n","        x_nint_test.append(np.concatenate([graph.nodes[user][representation_name], graph.nodes[item][representation_name]]))\n","\n","  else:\n","    x_train = operation(graph, representation_name, operator, dataset, df_train, df_train)\n","    x_int_test = operation(graph,representation_name, operator, dataset, df_train, df_test)\n","    x_nint_test = operation(graph, representation_name, operator, dataset, df_train, df_out)\n","\n","  return x_train, x_int_test, x_nint_test\n","\n","def evaluate_rec_sys(representation_name, path, file_name, hyperparams_gae, operator, dataset, train_test, df_out):\n","\n","  for kernel in define_kernels():\n","    if kernel != 'rbf':\n","      for gamma in define_gammas():\n","        for nu in define_nus():\n","          ocsvm = OCSVM(kernel=kernel,nu=nu,gamma=gamma)\n","          line_parameters = str(hyperparams_gae) + '_kernel:' + kernel + '_gamma:' + gamma + '_nu:' + str(nu)\n","          print(line_parameters)\n","          metrics = init_metrics()\n","\n","          for i in range(1,5):\n","            df_train, df_test = train_test[i]\n","            pickle = os.listdir(f\"{PATH_TO_GAE_GRAPHS}/{dataset}_{i}\")[0]\n","            with open(f\"{PATH_TO_GAE_GRAPHS}/rec_sys_{i}/{pickle}\", \"rb\") as file:\n","              G = pkl.load(file)\n","\n","              x_train, x_int_test, x_nint_test = extract_emb_from_graph(G, representation_name, operator, df_train, df_test, df_out)\n","\n","              start = time.time()\n","              values = evaluate_model(x_train, x_int_test, x_nint_test, ocsvm)\n","              end = time.time()\n","              time_ = end - start\n","              values['time'] = time_\n","              save_values(metrics, values)\n","\n","          write_results(metrics, file_name, line_parameters, path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QT9e5UddNXpk"},"outputs":[],"source":["import pickle as pkl\n","import networkx as nx\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","path_results = '/content/drive/MyDrive/USP/Doctorate/Research/Articles/Early Fusion For One-class Learning On Heterogeneous Graphs/resultados/'\n","\n","dataset = 'rec_sys'\n","\n","principal_node = ''\n","\n","folds = 5\n","kf = foldValidation(folds)\n","train_test = train_test_split_ocl_recommendation(kf, df_int)\n","\n","for operator in ['without', 'sum', 'sub', 'avg', 'min', 'max', 'multiply', 'concatenate']:\n","  print(operator)\n","  evaluate_rec_sys('f_features', path_results, dataset + '_' + operator + '.csv', 'reg', operator, dataset, train_test, df_out)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}